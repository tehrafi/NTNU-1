\documentclass[11pt, a4paper, norsk]{article}
\usepackage{math}
\begin{document}
	\begin{titlepage}
	    \centering
	    \vspace*{\fill}

	    \vspace*{0.5cm}

	    \huge
	    TMA4110 - Notater 2019

	    \vspace*{0.5cm}

	    \large Sander Lindberg

	    \vspace*{\fill}
	\end{titlepage}
    \tableofcontents

	\newpage

	\section{Komplekse tall}
	$Z = a + bi \quad Z \in \mathbb{C}$ 

	Skal bli kvitt at $x^2 + 1 = 0$ ikke har noen løsning.

	Vi kan nå skrive $x = \pm \sqrt{-1}$

	Finner opp et nytt tall: $i=\sqrt{-1} \implies i^{2} = -1$

	i kalles den \textit{imaginære enheten}
    

	\begin{Example}{}{}
	  Løs $x^{2} + 2x + 2 = 0$

	  \begin{align*}
	    a = 1 \\
	    b, c = 2 \\
	    \\
        x &= \frac{-2 \pm \sqrt{- 2*1*2}}{2} \\
	      &= \frac{-2 \pm \sqrt{4 - 8}}{2} \\
	      &= \frac{-2 \pm \sqrt{-4}}{2} = \frac{-1 \pm 2i}{2} = -1 \pm i \\
	  \end{align*} 
     \end{Example} 
     \begin{Formel}{}{}
         \begin{enumerate}
             \item $R = |z| = \sqrt{a^2 + b^2}$ kalles ''lengden'', ''modulus'' eller ''absolut verdi''.
             \item $arg(z) = \theta = \arcsin{\frac{b}{|z|}} = \arccos{\frac{a}{|z|}} = \begin{cases}
            \arctan{\frac{b}{a}}, a>0 \\
            \arctan{\frac{b}{a}} + \pi, a < 0 \\
            \frac{\pi}{2}, a=0, b>0 \\
            \frac{3\pi}{2}, a = 0, b < 0
    \end{cases} $
         \end{enumerate} Kalles ''argumentet'' til $z$.
     \end{Formel}
     \begin{Example}{}{}
         Gitt $z = -1 + i$ og $w = 1 + \sqrt{3}i$, finn $|z|$, $|w|$, $arg(z)$ og $arg(w)$:
         \\
         $|z| = \sqrt{1+1} = \sqrt{2}$ \\
         $|w| = \sqrt{4} = \sqrt{2}$ \\
         $arg(z) = \frac{3\pi}{2}$ ($\arctan{\frac{1}{-1}} + \pi$) \\
         $arg(w) = \frac{\pi}{3}$ ($\arctan{\sqrt{3}}$)
     \end{Example}
     \subsection{Polar form}%
     \label{sub:polar_form}
        Starter med å se på noen geometriske rekker:
        \begin{align*}
            e^{x} = 1 + x + \frac{x^2}{2} + \frac{x^{3}}{3} \dots \frac{x^{n}}{n}
            \\
            \\
            \cos{x} = 1 - \frac{x^2}{2!} + \frac{x^{4}}{4!} \dots
            \\
            \\
            \sin{x} = x - \frac{x^{3}}{3!} + \frac{x^{5}}{5!} \dots
        \end{align*}

        Disse kan settes sammen til Eulers formel:
        \begin{Definition}{Eulers Formel}{}
            $e^{ix} = 1 + ix + \frac{(ix)^{2}}{2!} + \frac{(ix)^{3}}{3!} \dots$
        \end{Definition}

        Vi kan nå skrive $z$ på polarform:

        \begin{align*}
            z &= a + bi \\
              &= r(\cos{\theta} + i\sin{\theta}) \\
              &= re^{i\theta}
        \end{align*}
        
        \subsection{Hvorfor komplekse tall?}%
        \label{sub:hvorfor_komplekse_tall_}
        
        \begin{Theorem}{Analysens fundamentalteorem}{}
        Polynomet $P(z) = z^{n} + a_{n-1}z^{n-1} + a_{n-2}z^{n-2} \dots a_1z+a_0$ kan allid faktoriseres $P(z) = \prod_{1}^{n} (z-z_{i})$ der $z_{i}$ er løsninger av likningen $P(z) = 0$
        \end{Theorem}

        \begin{Example}{}{}
            Faktoriser $z^2 + z + 1$ \\
            $z = \frac{-1 \pm \sqrt{1-4}}{2} = -\frac{1}{2} \pm \frac{\sqrt{3}}{2}i$
        \end{Example}
        En polynomlikning har alltid $n$ løsninger
        \begin{Example}{}{}
            $z^{3} -3z^2 + 3z -1 = (z-1)^{3}$ \\
            Sier at $z = 1$ er en rot med multiplisitet 3.
        \end{Example}

        Spesialtilfelle: 

        \begin{Example}{}{}
            Finn alle $z$ som tilfredsstiller $z^{n} = -1$ \\
            Komplekse n-te røtter: \\
            Skal løse $z^{n} = w$. $w$ er kjent, skal finne $z$. \\
            \begin{align*}
                z^{n} &= w = re^{i\theta} = re^{o(\theta + 2k\pi)} \\
                      &\implies z = \sqrt[n]{r} \cdot \sqrt[n]{e^{i(\theta + 2k\pi)}} \\
                      &= \sqrt[n]{r} \cdot \sqrt[n]{e^{i(\frac{\theta}{n} + \frac{2k\pi}{n})}}, \quad 0 \leq k \leq n-1
            \end{align*}
        \end{Example}
        \newpage
        \section{Lineære systemer og gausseliminasjon}
        \begin{Theorem}{}{}
            Et lineært likningsystem har enten
            \begin{enumerate}
                \item Entydig løsning
                \item Ingen løsning 
                \item Uendelig mange løsninger
            \end{enumerate}
        \end{Theorem}

        \begin{Example}{Linkingssystem med uendelig mange løsninger}{}
           \begin{align*}
               \begin{bmatrix}[ccc|c]
                    2 & 3 & 4 & 4 \\
                    3 & 4 & 5 & 5 \\
                    4 & 5 & 6 & 6
                    \end{bmatrix} &\sim \begin{bmatrix}[ccc|c]
                                        2 & 3 & 4 & 4 \\
                                        0 & 1 & 2 & 2 \\
                                        0 & 1 & 2 & 2
                                    \end{bmatrix}
                                    \\
                                    &\sim \begin{bmatrix}[ccc|c]
                                        2 & 3 & 4 & 4 \\
                                        0 & 1 & 2 & 2 \\
                                        0 & 0 & 0 & 0
                                    \end{bmatrix}
           \end{align*}
           Dette ser ut som tre likninger, men det er egt bare 2:
           \begin{align*}
            2x + 3y + 4z = 4 \\
            y + 2z = 2               
           \end{align*}

           Vi har to likninger med tre ukjente, som ikke går an å løse. Vi setter derfor inn frie variabler. (finne en parametisering)
           \begin{enumerate}
               \item Velg $z = s$
               \item Skriv de andre ved hjelp av $s$ ($x$ og $y$)
               \item Skriv likningene på vektorform
           \end{enumerate}
           Velger $z=s$, da får vi $y = 2-2s$ og $x = -2+s$. På vektorform blir dette:
           \begin{align*}
               \begin{bmatrix}
                   x \\
                   y \\
                   z
                   \end{bmatrix} \begin{bmatrix}
                                    -2 + s \\
                                    2 - 2s \\
                                    s
                                    \end{bmatrix} = \begin{bmatrix}
                                                        -2 \\
                                                        2 \\
                                                        0
                                                        \end{bmatrix} + \begin{bmatrix}
                                                        1 \\
                                                        -2 \\
                                                        1
                                                    \end{bmatrix}\cdot s
           \end{align*}
        \end{Example}
        \subsection{Vektorer}%
        \label{sub:vektorer}
        
        En vektor $\underline{x}$ skrives på formen $\underline{x} = \begin{bmatrix}
            x_1 \\
            x_2 \\
            x_3 \\
            . \\
            x_{n}
        \end{bmatrix} \in \mathbb{R}$

        \begin{Formel}{To operasjoner på vektorer}{}
            \textbf{Addisjon}:

            $\underline{x} + \underline{y} = \begin{bmatrix}
                x_1 \\
                x_2 \\
                x_3
            \end{bmatrix} + \begin{bmatrix}
                y_1 \\
                y_2 \\
                y_3
            \end{bmatrix} = \begin{bmatrix}
                x_1 + y_1 \\
                x_2 + y_2 \\
                x_3 + y_3
            \end{bmatrix}$

            \textbf{Skalarmultiplikasjon}:
            
            $k\cdot \underline{x} = k \cdot \begin{bmatrix}
                x_1 \\
                x_2 \\
                x_3
            \end{bmatrix} = \begin{bmatrix}
                kx_1 \\
                kx_2 \\
                kx_3
            \end{bmatrix}$
        \end{Formel}
        
        \begin{Formel}{Skalarprodukt og vektorprodukt}{}
            \textbf{Skalarprodukt}:

            $\underline{x} \cdot \underline{y} = x_1y_1 + x_2y_2 + x_3y_3 \dots$

            \textbf{Vektorprodukt}:

            $\underline{x} \times \underline{y} = \begin{bmatrix}
                x_2y_2 - x_3y_2 \\
                x_3y_1 - x_1y_3 \\
                x_2y_1 - x_1y_2
            \end{bmatrix}$
        \end{Formel}

        \subsubsection{Lineærkombinasjon}
        
        En lineærkombinasjon av vektorer skrives på formen:
        $\underline{x_1}k_1 + \underline{x_2}k_2 + \dots + \underline{x_n}k_{n}$

        Det er altså en vektor du får ved å legge sammen to vektorer multiplisert med en skalar. 
        
        \newpage
       \section{Vektorlikninger}
        
       Litt repitisjon:

       Søylevektor: $\underline{x} = \begin{bmatrix}
           x_1 \\
           x_2 \\
           x_3
       \end{bmatrix}$

       $\underline{x} + \underline{y} = \begin{bmatrix}
           x_1 \\
           x_2 \\
           x_3
       \end{bmatrix} + \begin{bmatrix}
           y_1 \\
           y_2 \\
           y_3
       \end{bmatrix} = \begin{bmatrix}
           x_1 + y_1 \\
           x_2 + y_2 \\
           x_3 + y_3
       \end{bmatrix}$

       $c \cdot \begin{bmatrix}
           x_1 \\
           x_2 \\
           x_3
       \end{bmatrix} = \begin{bmatrix}
           cx_1 \\
           cx_2 \\
           cx_3
       \end{bmatrix}$ 

        Lineærkombinasjon av $\underline{x}$ og $\underline{y}$: $a\underline{x} + b\underline{y}$.  
    
        $\underline{x}$ og $\underline{y}$ er vektorer i $\mathbb{R}^{n}$ ($\mathbb{C}^{n}$)
    \subsection{Linære spenn}%
    \label{sub:linaere_spenn}
    
        \begin{Definition}{Lineært spenn}{}
            $Sp\left\{\underline{x}, \underline{y}\right\}$ kalles det \textit{lineære spennet}.
            $Sp\left\{\underline{x}, \underline{y}\right\} = \left\{a\underline{x} + b\underline{y} \mid a, b \in \mathbb{R} \right\}$
            
            Det lineæret spennet er altså mengden av alle linærkombinasjoner til $\underline{x}$ og $\underline{y}$. 
        \end{Definition}

        \begin{Example}{}{}
            Hva er $Sp\left\{\begin{bmatrix}
                                1 \\
                                0
                            \end{bmatrix}\right\}$?

                            Vet at $Sp\left\{\underline{x}\right\} = Sp\left\{a\underline{x}\right\}$ som i mitt tilfelle er $Sp\left\{a \begin{bmatrix}
                                            1 \\
                                            0
                                        \end{bmatrix}\right\} = \left\{\begin{bmatrix}
                                                                            a \\
                                                                            0
                                                                        \end{bmatrix}\right\}$

            Som tilsvarer x-aksen.
            Hva er $Sp\left\{\begin{bmatrix}
                1 \\
                0 \\
                0
            \end{bmatrix}, \begin{bmatrix}
                0 \\
                1 \\
                0
            \end{bmatrix}\right\}$?

            Bruker samme metode som sist:
            $Sp\left\{a \begin{bmatrix}
                1 \\
                0 \\
                0
            \end{bmatrix} + b \begin{bmatrix}
                0 \\
                1 \\
                0
            \end{bmatrix}\right\} = \left\{\begin{bmatrix}
                a \\
                b \\
                0
            \end{bmatrix}\right\}$
            Som er xy-planet.
        \end{Example}

        \subsection{Linkningssett}%
        \label{sub:linkningssett}
        
        Vi er nå interessert i å løse likningsett, f.eks:
        
        \begin{align*}
            \begin{cases}
                x + 2y - 2z = -5 \\
                x + 5y + 9z = 33 \\
                2x + 5y -z = 0
            \end{cases} \quad = x \cdot \begin{bmatrix}
                1 \\
                1 \\
                2
            \end{bmatrix} + y \cdot \begin{bmatrix}
                2 \\
                5 \\
                5
            \end{bmatrix} + z \cdot \begin{bmatrix}
                -2 \\
                9 \\
                -1
            \end{bmatrix} = \begin{bmatrix}
                -5 \\
                33 \\
                0
            \end{bmatrix}
        \end{align*}
            
        Setter opp som en matrise og gausseliminerer for å finne løsning:
        \begin{align*}
            \begin{bmatrix}[ccc|c]
                1 & 2 & -2 & -5 \\
                1 & 5 & 9 & 33 \\
                2 & 5 & -1 & 0
            \end{bmatrix} &\sim \begin{bmatrix}[ccc|c]
            1 & 2 & -2 & -5 \\
            1 & 5 & 9 & 33 \\
            0 & 1 & 3 & 10
            \end{bmatrix}
            \\
            &\sim \begin{bmatrix}[ccc|c]
                1 & 2 & -2 & -5 \\
                0 & 3 & 11 & 38 \\
                0 & 1 & 3 & 10
            \end{bmatrix}
            \\
            &\sim \begin{bmatrix}[ccc|c]
                1 & 2 & -2 & -5 \\
                0 & 1 & \frac{11}{3} & \frac{38}{7} \\
                0 & 0 & 1 & 4
            \end{bmatrix}
            \\
            &\sim \begin{bmatrix}[ccc|c]
                1 & 2 & -2 & -5 \\
                0 & 1 & 0 & -2 \\
                0 & 0 & 1 & 4
            \end{bmatrix}
            \\
            &\sim \begin{bmatrix}[ccc|c]
                1 & 0 & 0 & 7 \\
                0 & 1 & 0 & -2 \\
                0 & 0 & 1 & 4
            \end{bmatrix}
        \end{align*}
        Ser at $x = 7$, $y = -2$ og $z = 4$. Setter inn i det originale utrykket for å sjekke:
        \begin{align*}
            7 + 2 \cdot -2 - 2\cdot 4 &= -5 \\
            7 + 5 \cdot -2 + 9 \cdot 4 &= 33 \\
            2 \cdot 7 + 5 \cdot -2 - 4 &= 0
        \end{align*}
        Jeg har funnet riktige verdier for $x$, $y$ og $z$.

        
        \begin{Definition}{Matriser}{}
            En $m$x$n$ matrise skrives på formen:
            \begin{align}
                \begin{bmatrix}
                    a_{11} & a_{12} & a_{1n} \\
                    a_{21} & a_{22} & a_{2n} \\
                    a_{mn} & a_{m2} & a_{mn}
                \end{bmatrix}
            \end{align}
            Denne matrisen har altså $m$ rader og $n$ kolonner.
        \end{Definition}

        \begin{Definition}{Søylevektorer og radvektorer}{}
            Hvis $\underline{v}$ er en søylevektor, kan den skrives på formen:
            $\underline{v} = \begin{bmatrix}
                v_1 \\
                v_2 \\
                v_{n}
            \end{bmatrix}$

            En radvektor er basically bare en liggende søylevektor:
            $\underline{w} = \begin{bmatrix}
                w_1 & w_2 & w_3
            \end{bmatrix}$
        \end{Definition}


        \begin{Formel}{Fremgangsmåte for vektorlikniger}{}
            Dette er vel egt ikke en formel, men det går fint. Her kommer en oppskrift på hvordan løse vektorlikninger:
            \begin{enumerate}
                \item Sett opp de forskjellige søylevektorene som kolonner i en matrise.
                \item Gausseliminer.
            \end{enumerate}
            Ikke så veldig mye verre enn det egt.
        \end{Formel}


        \begin{Formel}{Gange matrise med vektor}{}
            Igjen, ikke formel, men vet ikke hva jeg skal putte det under. 

            (Har hatt et par glass vin når jeg skriver dette, så ikke hat meg om noe er feil <3 )
            \\
            \\
            Gitt $A=\begin{bmatrix}
                \underline{v_1} & \underline{v_2} & \underline{v_3} & \dots & \underline{v_{n}}
            \end{bmatrix}$, hvor $A$ er en $m$x$n$ matrise og $\underline{x} = \begin{bmatrix}
                x_1 \\
                x_2 \\
                x_3 \\
                \dots \\
                x_{n}
            \end{bmatrix}$

            Så er $A \cdot \underline{x} = \begin{bmatrix}
                v_1 & v_2 & \dots & v_{n}
            \end{bmatrix} \cdot \begin{bmatrix}
                x_1 \\
                x_2 \\
                \dots \\
                x_{n}
            \end{bmatrix} = x_1v_1 + x_2v_2 + x_3v_3 + \dots + x_{n}v_{n} \in \mathbb{R}^m$ 
        \end{Formel}

        \begin{Example}{Matrise ganger vektor}{}
            Gitt $A = \begin{bmatrix}
                5 & 0 & -2 \\
                3 & 1 & 4
            \end{bmatrix}$ og $\underline{x} = \begin{bmatrix}
                2 \\
                -1 \\
                3
            \end{bmatrix}$ finn $A \cdot \underline{x}$

            Setter opp og ganger:
            \begin{align*}
                \begin{bmatrix}
                    5 & 0 & -2 \\
                    3 & 1 & 4
                \end{bmatrix} \cdot \begin{bmatrix}
                    2 \\
                    -1 \\
                    3
                \end{bmatrix} &= 2 \cdot \begin{bmatrix}
                    5 \\
                    3
                \end{bmatrix} + (-1) \cdot \begin{bmatrix}
                0 \\
                1
                \end{bmatrix} + 3 \cdot \begin{bmatrix}
                    -2 \\
                    4
                \end{bmatrix}
                \\
                &= \begin{bmatrix}
                    4 \\
                    17
                \end{bmatrix}
            \end{align*}
        \end{Example}
        
        \begin{Theorem}{}{}
            $A(\underline{v} + \underline{w}) = A\underline{v} + A\underline{w}$

            $A(c\underline{v}) = c(A\underline{v})$
        \end{Theorem}
        
        \subsection{Enhetsvektorer}%
        \label{sub:enhetsvektorer}
        
        Enhetsvektorer er vektorer som har én 1er og resten 0. De finnes i alle $\mathbb{R}^n$.
       
        Enhetsvektorer i $\mathbb{R}^2$:
        $\underline{e_1} = \begin{bmatrix}
            1 \\
            0
        \end{bmatrix}, \quad \underline{e_2} = \begin{bmatrix}
            0 \\
            1
        \end{bmatrix}$

        Enhetsvektorer i $\mathbb{R}^3$:
        $\underline{e_1} = \begin{bmatrix}
            1 \\
            0 \\ 
            0
        \end{bmatrix}, \quad \underline{e_2} = \begin{bmatrix}
            0 \\
            1 \\ 
            0
        \end{bmatrix}, \quad \underline{e_3} = \begin{bmatrix}
            0 \\
            0 \\
            1
        \end{bmatrix}$

        Samme mønster for $\mathbb{R}^n$

        \subsection{Vektorlikninger}%
        \label{sub:vektorlikninger}
        
        \begin{Definition}{En vektorlikgning}{}
            En vektorlikning er på formen:
            \begin{align*}
                \begin{bmatrix}
                    a_{11} \\
                    a_{21} \\
                    a_{m1}
                \end{bmatrix} \cdot x_1 + \dots + \begin{bmatrix}
                    a_{1n} \\
                    a_{2n} \\
                    a_{mn} \\
                    \end{bmatrix} \cdot x_{n} = \begin{bmatrix}
                        b_1 \\
                        b_2 \\
                        b_{n}
                    \end{bmatrix}
            \end{align*}
            Vi skriver den som regel om til en matrise:
            \begin{align*}
                \begin{bmatrix}
                    a_{11} & a_{12} & \dots & a_{1n} \\
                    a_{21} & a_{22} & & a_{2n} \\
                    \dots & & & \\
                    a_{m1} & a_{m2} & \dots & a_{mn}
                \end{bmatrix} \cdot \begin{bmatrix}
                    x_1 \\
                    x_2 \\
                    \dots \\
                    x_{n}
                \end{bmatrix} = \begin{bmatrix}
                    b_1 \\
                    b_2 \\
                    \dots \\
                    b_{n}
                \end{bmatrix}
            \end{align*}
            
        \end{Definition}
        
        
        \section{Forelesning 16.10.19}
        I dag:

        \begin{itemize}
            \item Oppsummere projeksjon
            \item ''Anvendelse av projeksjon''
            \item Kapittel 10.
        \end{itemize}
                
        \begin{Formel}{}{}
            \begin{align*}
                P_{\underline{v}} &= \frac{<\underline{v}, \underline{w}>}{<\underline{v}, \underline{v}>} \cdot \underline{v}
                \\
                \underline{\mathbb{R}^n}&: <\underline{v}, \underline{w}> = \underline{v}^{T}\underline{w} \\      
                \underline{\mathbb{C}^n}&: <\underline{v}, \underline{w}> = \underline{v}^{*}\underline{w} \\
                \underline{C([a,b])}&: <f, g> = \frac{1}{b-a}\int_{a}^{b}f(x)g(x)dx
            \end{align*}
        \end{Formel}

        \subsection{Fourier-analyse(Anvendelse)}%
        \label{sub:fourier_analyse}
        
        $\left\{1, \cos{x}, \sin{x}, \cos{2x}, \sin{2x} \dots \right\}$ er en ortiginal basis. Tenker at vi har en funksjon som går som et signal (ujevne bølger opp og ned, radiobølger f.eks). Har lyst til å finne ut av hvordan bølgene oppfører seg. Og det var det? Han ville visst bare nevne det.

        \section{Egenverdier og egenvektorer}
        
        Det vi har jobbet med til nå er linærtransformasjoner, alle lineærtrans kan representeres som martriser. F.eks $f_{A}(\underline{v})= A\underline{v}$.

        Hva om $f_{A}(\underline{v}) = \lambda\underline{v}$, for $\lambda \in \mathbb{R}$? Alstå, vi får bare et tall ganget med en vektor? Kaller $\lambda$ for egenverdi og $\underline{v}$ for egenvektor. 
        \begin{Example}{}{}
                        \begin{align*}
                A = \begin{bmatrix}
                    1 & 3 \\
                    4 & -3
                \end{bmatrix}, e_{1}, e_{2}, \underline{u} = \begin{bmatrix}
                    -1 \\
                    -2 \\
                \end{bmatrix}, \underline{v} = \begin{bmatrix}
                    3 \\
                    2
                \end{bmatrix}
                \\
                A\underline{e_1} &= \begin{bmatrix}
                    1 \\
                    4
                \end{bmatrix}
                \\
                A\underline{e_2} &= \begin{bmatrix}
                    3 \\
                    -3
                \end{bmatrix}
                \\
                A\underline{u} &= \begin{bmatrix}
                    -7 \\
                    2
                \end{bmatrix}
                \\
                A\underline{v} &= \begin{bmatrix}
                    9 \\
                    6
                \end{bmatrix} = 3\underline{v}
            \end{align*}
            Ser at det er kun en av vektorene ($\underline{v}$) som gir en skalering. Vi har lyst til å finne akkurat de tallene som gir skalering og de vektorene detgjelder.
        \end{Example}

        \begin{Definition}{}{}
            La $A$ være en $n$x$n$ matrise, $\underline{v}$ være en $n$x$1$ vektor og $\lambda$ være en skalar.

            Dersom $A\underline{v} = \lambda \underline{v}$, sp kalles $\lambda$ en egenverdi av $A$ og $\underline{v}$ en egenvektor av $A$, tilhørende $\lambda$
        \end{Definition}

        Hva om $\underline{w} = c\underline{v}$?

        Da er $A\underline{w} = A(c\underline{v}) = cA\underline{v} = c\lambda\underline{v} = \lambda c\underline{v} = \underline{w}$.

        Det følger at $\underline{w}$ også er en egenvektor til $A$ med hensyn til $\lambda$. 


        Hva om $\lambda = 0$? Det betyr at $A\underline{v} = \underline{0}$ Som vil si at kolonnene i $A$ \textit{ikke} er lineært uavhengige. Som videre vil si at $A$ ikke har noen invers. Determinanten er også $0$.

        \begin{Theorem}{}{}
            $\lambda = 0$ er en egenverdi til $A$ $\iff$ $A$ ikke er invertibel. 
        \end{Theorem}

        \begin{Theorem}{}{}
            La $T:V\rightarrow V$ (vi har en kvadratisk matrise) være en lineærtransformasjon. (Som også betyr at $V$ er et vektorrom), og la $\lambda_{1} \neq \lambda_{2} \neq \dots \neq \lambda_{n}$ være egenverdier til $T$. Da er de tilhørende egenvektorene $\underline{v_{1}}, \underline{v_2}, \dots ,\underline{v_{t}}$ lineært uavhengige.
        \end{Theorem}

        \begin{Theorem}{}{}
            La $T:V\rightarrow V$ være en lineærtransformasjon representert ved $n$x$n$ matrise $A$. La $A$ ha $n$ distinkte egenverdier. 

            Da utgjør de tihlørende egenvektorene $\underline{v_1}, \underline{v_2} \dots ,\underline{v_{n}}$ en basis for $V$. 
        \end{Theorem}

        \begin{Example}{}{}
            \begin{align*}
                A &= \begin{bmatrix}
                    0 & 1 \\
                    1 & 0
                \end{bmatrix}
                \\
                A\underline{v} &= \begin{bmatrix}
                    v_2 \\
                    v_1
                \end{bmatrix}
            \end{align*}
            Anta en linje gjennom origo ($y = x$) da er denne matrisemultiplikasjonen en speiling over denne linjen. 

            Si vi har en vektor $\underline{v_1} = \begin{bmatrix}
                1 \\
                1
            \end{bmatrix}$. Ganger vi denne med $A$ får vi den samme vektoren ut igjen, med egenverdi $1$. ($A\underline{v_1} = 1\underline{v}$)

            Har vi linjen $y = -x$ og vektoren $\underline{v_3} = \begin{bmatrix}
                1 \\
                -1
            \end{bmatrix}$ får vi $A\underline{v_3} = \begin{bmatrix}
                -1 \\
                1
            \end{bmatrix} = (-1)\underline{v_3}$ (Egenvedien er $-1$)
        \end{Example}
    
        \begin{Definition}{}{}
            La $\lambda$ være en egenverdi til $A$ med tilhørende egenvektor $\underline{v}$, da kalles mengden $Sp\left\{\underline{v}\right\}$ egenrommet til $\lambda$. Nullvektoren er i egenrommet, selvom nullvektoren ikke er en egenvektor. (super forvirrende :p)
        \end{Definition}

        \subsection{Metode for å finne egenverdi og egenvektor}%
        \label{sub:metode_for_a_finne_egenverider_og_egenvektorer}
        
        $A\underline{v} = \lambda \underline{v} \implies A\underline{v} - \lambda\underline{v} = 0$.

        Vet at $AI_{n} = A$ og $\underline{v}I_{n} = \underline{v}$ 

        Ved hjelp av dette kan vi finne $A\underline{v} - \lambda \underline{v} = 0 \implies A\underline{v} - \lambda I_{n}\underline{v} = 0 \implies (A-\lambda I_{n}) \underline{v} = \underline{0} \implies A - \lambda I_{n}$ ikke er invertibel.

        Dette impliserer også et $det(A - \lambda I_{n}) = 0$
        Videre, $\underline{v}$ ligger i nullrommet til $A$ fordi vi ganget med et matrise og fikk $\underline{0}$

        \begin{Theorem}{}{}
            La $A$ være en $m$x$n$ matrise. 

            Da:
            \begin{itemize}
                \item Egenverdiene til A er løsningene $\lambda$ til $det(A - \lambda I_{n}) = 0$
            \item $\lambda$ egenverdi til $A$, så er egenvektorene $\underline{v}$ til $\lambda$ løsningene til $(A - \lambda I_{n})\underline{v} = 0$ 
            \end{itemize}
            God, skjønner ingenting av dette her :p
        \end{Theorem}

        \begin{Definition}{}{}
            Dimensjonen til egenrommet til $\lambda$ kalles den geometriske multiplisiteten til $\lambda$. 
        \end{Definition}

        \begin{Example}{}{}
            \begin{align*}
                A = \begin{bmatrix}
                    1 & 3 \\
                    4 & -3
                \end{bmatrix}
            \end{align*}
            Vil finne egenverdiene.
            \begin{align*}
                A - \lambda I_{2} &= \begin{bmatrix}
                    1 & 3 \\
                    4 & -3
                \end{bmatrix}- \begin{bmatrix}
                    \lambda & 0 \\
                    0 & \lambda
                \end{bmatrix} = \begin{bmatrix}
                    1-\lambda & 3 \\
                    4 & -3-\lambda
                \end{bmatrix}
                \\
                \\
                    det(A-\lambda I_{2}) &= (1-\lambda)(-3-\lambda) - 12
                    \\
                                         &= -3 - \lambda + 3\lambda + \lambda^2 - 12
                                         \\
                                         &= \lambda^2 + 2\lambda - 15 = 0
                                         \\
                                         &\implies \lambda = \frac{-2 \pm \sqrt{2^2 - 4(-15)}}{2}
                                         \\
                                         &\implies \lambda = -1 \pm 4
                                         \\
                                         \lambda_{1} = 3, \quad \lambda_{2} = -5
            \end{align*}
            Vi kan nå finne egenvektorene til $A$. 

            \begin{align*}
                A - 3I_{2} = \begin{bmatrix}
                    -3 & 3 \\
                    4 & -6
                \end{bmatrix} &\sim \begin{bmatrix}
                -2 & 3 \\
                0 & 0
                \end{bmatrix}
                \\
                &\implies \underline{v_1} = \begin{bmatrix}
                    3 \\
                    2
                \end{bmatrix}
                \\
                \\
                A + 5I_{2} = \begin{bmatrix}
                    6 & 3 \\
                    4 & 2
                \end{bmatrix} &\sim \begin{bmatrix}
                2 & 1 \\
                0 & 0
                \end{bmatrix}
                \\
                &\implies \underline{v_{2}} = \begin{bmatrix}
                    -1 \\
                    2
            \end{bmatrix}
            \end{align*}
            $\underline{v_1}$ er løsningen på $A\underline{v_1} = 0$ og  $\underline{v_2}$ er løsningen på $A\underline{v_2} = 0$ 
            
            Egenrommet:
            $\lambda = 3$ er $Sp\left\{\begin{bmatrix}
                3 \\
                2
            \end{bmatrix}\right\}$ og til $\lambda = -5$ er $Sp\left\{\begin{bmatrix}
                -1 \\
                2
            \end{bmatrix}\right\}$
        \end{Example}

        \begin{Example}{}{}
            \begin{align*}
                A &= \begin{bmatrix}
                    -8 & 0 & 6 \\
                    12 & 4 & -6 \\
                    -20 & 0 & 14
                \end{bmatrix}
                \\
                    A - \lambda I_{3} &= \begin{bmatrix}
                    -8 - \lambda & 0 & 6 \\
                    12 & 4 - \lambda & -6 \\
                    -20 & 0 & 12 - \lambda
                \end{bmatrix}
                \\
                \\
                    det(A - \lambda I_3) &= (-8 - \lambda)(4-\lambda)(14-\lambda) + (4-\lambda)\cdot 20\cdot 6 = 0
                    \\
                                         &= (4-\lambda)((-8-\lambda)(14-\lambda) + 120) \\
                                         &= (4-\lambda)(\lambda^2-6\lambda+8) = 0
                                         \\
                                         &\implies \lambda_{1} = 4, \quad \lambda = \frac{6 \pm \sqrt{36-32}}{2} 
                                         \\
                                         &\implies \lambda_{2} = 3+1 = 4, \quad \lambda_{3} = 3-1 = 2
            \end{align*}            
            Har lyst til å vinne egenvektorer:
            \begin{align*}
                A - 4I_{3} = \begin{bmatrix}
                    -12 & 0 & 6 \\
                    12 & 0 & -6 \\
                    -20 & 0 & 10
                \end{bmatrix} &\sim \begin{bmatrix}
                -2 & 0 & 1 \\
                0 & 0 & 0 \\
                0 & 0 & 0
                \end{bmatrix}
                \\
                &\implies \underline{v_1} = \begin{bmatrix}
                    0 \\
                    1 \\
                    0
                \end{bmatrix}, \quad \underline{v_2} = \begin{bmatrix}
                    1 \\
                    0 \\
                    2
                \end{bmatrix}
                \\
                \\
                A - 2I_{3} = \begin{bmatrix}
                    -10 & 0 & 6 \\
                    12 & 2 & -6 \\
                    -20 & 0 & 12
                \end{bmatrix} &\sim \begin{bmatrix}
                -5 & 0 & 3 \\
                6 & 1 & -3 \\
                0 & 0 & 0
                \end{bmatrix}
                \\
                &\sim \begin{bmatrix}
                    -5 & 0 & 3 \\
                    1 & 1 & 0 \\
                    0 & 0 & 0
                \end{bmatrix}
                \\
                &\implies \underline{v_3} = \begin{bmatrix}
                    1 \\
                    -1 \\
                    \frac{5}{3}
                \end{bmatrix}
            \end{align*}
            Kan også bruke metoder fra tidligere (frie variabler) for å finne nullrommet (som er egenvektorene) (tror jeg).

            Egenrommet til $\lambda = 4$ har dimensjon $2$ og til $\lambda = 2$ har dimensjon $1$. 
        \end{Example}

        \section{Forelesning 17.10.19}

        \subsection{Egenverdier og egenvektorer}%
        \label{sub:egenverdier_og_egenvektorer}
        
        $A\underline{v} = \lambda \underline{v}$, der $A$ er en $m$x$n$ matrise. 

        Finne $\lambda$: Løs $det(A - \lambda I_{n}) = 0$.

        Finne $\underline{v}$: Gitt $\lambda, \underline{v} \in Null(A-\lambda I_{n})$ 

        Geometrisk multiplisitet til $\lambda$ er dimensjonen til egenrommet til $\lambda$. 

        \begin{Example}{}{}
            \begin{align*}
                A &= \begin{bmatrix}
                    -8 & 0 & 6 \\
                    12 & 4 & -6 \\
                    -20 & 0 & 14
                \end{bmatrix}, \lambda_{1} = \lambda_{2} = 4, \lambda_{3} = 2
                \\
                \\
                \text{Da er:}
                \\
                    \underline{v_1} &= \begin{bmatrix}
                    0 \\
                    1 \\
                    0
                \end{bmatrix}
                \\
                        \underline{v_2} &= \begin{bmatrix}
                    1 \\
                    0 \\
                    2
                \end{bmatrix}
                \\
                            \underline{v_3} &= \begin{bmatrix}
                    3 \\
                    -3 \\
                    5
                \end{bmatrix}
            \end{align*}
        \end{Example}
       
        \section{Eksistens av egenverdier og egenvektorer}
        
        Lurer på:
        \begin{itemize}
            \item Hvor mange egenverdier kan en matrise ha?
            \item Finnes det alltid egenvektorer ($\underline{v}$) og evgenverdier ($\lambda$)?
            \item  Hva er dimensjonene til egenrommene? (Generelt, kan alltid regne det ut.)
            \item Kan vi finne en basis for $\mathbb{R}^{n}$ eller $\mathbb{C}^{n}$
bestående av egenvektorer?
        \end{itemize}
        
        \begin{Example}{Generell egenverdi 2*2 matrise}{}
            \begin{align*}
                A &= \begin{bmatrix}
                    a & b \\
                    c & d
                \end{bmatrix}
                \\
                  &\implies det(A - \lambda I_{n})
                  \\
                  &= (a-\lambda)(d-\lambda) - bc
                  \\
                  &= \lambda^2 - a\lambda - d\lambda + ad - bc
            \end{align*}
        \end{Example}

        Hvor mange løsninger har et generelt polynom?
        
        \begin{Theorem}{Algebraens fundamentalteorem}{}
            Dersom vi har en ligning $a_{n} x^n + a_{n-1}x^{n-1} + \dots + a_{1}x + a_{0}$ kan den skrives som $a_{n}\prod_{i=1}^{n}(x-x_{i})$ hvor $x_{i}$ er løsninger av $f(x) = 0$

            $\implies \forall a_{i} \in \mathbb{R}$ eller $\forall a_{i} \in \mathbb{C}$, finnes det $n$ (ikke nødvendigvis unike) komplekse løsninger $x_{i}$ av $f(x) = 0$.

            Dersom faktoren $(x-x_{i})$, for en gitt $i$ forekommer $m$ ganger, sier vi at $x_{i}$ har multiplisitet $m$.
        \end{Theorem}

        \begin{Theorem}{}{}
            En kompleks $n$x$n$ matrise $A$ har alltid $n$ egenverdier, når vi teller med multiplisiteten. (Hvis en egenverdi oppstår to ganger, teller vi dette som 2).
        \end{Theorem}
        
        \begin{Definition}{Algebraisk multiplisitet}{}
            Dersom $\lambda$ er en rot av $det(A - \lambda I_{n}) = 0$ og har multiplisitet $m$, så kaller vi $m$ den algebraiske multiplisiteten til $\lambda$. 
        \end{Definition}

        \begin{Theorem}{}{}
            En reell matrise $A$ ($n$x$n$) trenger ikke nødvendigvis ha noen reelle røtter (egenverdier). Dersom $n$ er et oddetall, vil vi \textit{alltid} ha minst én reell egenverdi.
        \end{Theorem}

        \begin{Theorem}{}{}
            La $\lambda$ være en kompleks egenverdi av $A$. 
            \\ \\
            Da er også $\bar{\lambda}$ en egenverdi. Hvis $\underline{v}$ egenvektor til $\lambda$, så er $\bar{\underline{v}}$ egenvektor til $\bar{\lambda}$
        \end{Theorem}

        \begin{Example}{}{}
            La $A = \begin{bmatrix}
                0 & -1 \\
                1 & 0
            \end{bmatrix}$ Hva er egenverdien(e) og egenvektor(ene)?

            Egenverdiene er gitt ved $det(A - \lambda I_{n}) = 0$:

            \begin{align*}
                det(A - \lambda I_{n}) &= det \left(\begin{bmatrix}
                    0 & -1 \\
                    1 & 0
                \end{bmatrix} - \begin{bmatrix}
                    \lambda & 0 \\
                    0 & \lambda
            \end{bmatrix} \right) = 0
            \\
                                       &= det\left(\begin{bmatrix}
                                               -\lambda & -1 \\
                                               1 & -\lambda
                                       \end{bmatrix} \right)= 0
                                       \\
                                       &= -\lambda \cdot -\lambda - -1 \cdot 1 = 0
                                       \\
                                       &= \lambda^2 + 1 = 0
                                       \\
                                       &\implies \lambda_{1} = i, \quad \lambda_{2} = -i
            \end{align*}

            Egenvektorer:
            \begin{align*}
                A - iI_{2} = \begin{bmatrix}
                    -i & -1 \\
                    1 & -i
                \end{bmatrix} &\sim \begin{bmatrix}
                -i & -1 \\
                -i & -1
                \end{bmatrix}
                \\
                &\sim \begin{bmatrix}
                    -i & -1 \\
                    0 & 0
                \end{bmatrix}
                \\
                \\
                &\implies \underline{v_1} = \begin{bmatrix}
                    1 \\
                    -i
                \end{bmatrix}
                \\
                \\
                A + iI_{2} = \begin{bmatrix}
                    i & -1 \\
                    1 & i
                \end{bmatrix} &\sim \begin{bmatrix}
                i & -1 \\
                0 & 0
                \end{bmatrix}
                \\
                \\
                &\implies \underline{v_2} = \begin{bmatrix}
                    1 \\
                    i
                \end{bmatrix}
            \end{align*}
            Vi ser at $\lambda_{1} = \bar{\lambda_2}$ og $\underline{v_1} = \bar{\underline{v_2}}$
        \end{Example}
        
        \begin{Example}{}{}
            Vi har $A = \begin{bmatrix}
                3 & 0 & 0 \\
                0 & 1 & -1 \\
                0 & 1 & 1
            \end{bmatrix}$. Har lyst til å finne egenvektorer og egenverdier.
            \begin{align*}
                A - \lambda I_{3} &= \begin{bmatrix}
                    3 & 0 & 0 \\
                    0 & 1 & -1 \\
                    0 & 1 & 1
                \end{bmatrix} - \begin{bmatrix}
                    \lambda & 0 & 0 \\
                    0 & \lambda & 0 \\
                    0 & 0 & \lambda
                \end{bmatrix}
                \\
                                  &= \begin{bmatrix}
                                      3 - \lambda & 0 & 0 \\
                                      0 & 1 - \lambda & -1 \\
                                      0 & 1 & 1-\lambda
                                  \end{bmatrix}
                                  \\
                                  &\implies det(A - \lambda I_3) = (3-\lambda)(1 - \lambda)(1 - \lambda) + (3-\lambda)
                                  \\
                                  &= (3-\lambda)((1-\lambda)^2 + 1) = (3-\lambda)(\lambda^2-2\lambda+2) = 0 \\
                                  &\implies \lambda_1 = 3
                                  \\
                                  \\
                    \lambda &= \frac{2 \pm \sqrt{4-8}}{2} 
                    \\
                            &= \frac{2 \pm 2i}{2} = 1 \pm i
                            \\
                            &\implies \lambda_2 = 1+i \quad \lambda_3 = 1-i
            \end{align*}
            La oss nå finne egenvediene:
            \begin{align*}
                A - 3I_3 = \begin{bmatrix}
                    0 & 0 & 0 \\
                    0 & -2 & -1 \\
                    0 & 1 & -2
                \end{bmatrix} &\sim \begin{bmatrix}
                0 & 0 & 0 \\
                0 & 0 & -5 \\
                0 & 1 & -2
                \end{bmatrix}
                \\
                &\sim \begin{bmatrix}
                    0 & 0 & 0 \\
                    0 & 0 & 1 \\
                    0 & 1 & 0
                \end{bmatrix}
            \end{align*}
            Vi har to pivotkolonner, aka 1 fri variabel. Vi får $\underline{v_1} = \begin{bmatrix}
                1 \\
                0 \\
                0
            \end{bmatrix}$
            Videre:
            \begin{align*}
                A - (1+i)I_3 = \begin{bmatrix}
                    2 - i & 0 & 0 \\
                    0 & -i & -1 \\
                    0 & 1 & -i
                \end{bmatrix} &\sim \begin{bmatrix}
                2-i & 0 & 0 \\
                0 & 0 & 0 \\
                0 & 1 & -i
                \end{bmatrix}
                \\
                &\implies \underline{v_2} = \begin{bmatrix}
                    0 \\
                    i \\
                    1
                \end{bmatrix} \implies \underline{v_3} = \begin{bmatrix}
                    0 \\
                    -i \\
                    1
                \end{bmatrix}
            \end{align*}
        \end{Example}

        \begin{Example}{}{}
            Vi har $A = \begin{bmatrix}
                1 & 1 & 0 \\
                0 & 1 & 0 \\
                0 & 0 & 2
            \end{bmatrix}$
            \begin{align*}
                A - \lambda I_3 &= \begin{bmatrix}
                    1 - \lambda & 1 & 0 \\
                    0 & 1 - \lambda & 0 \\
                    0 & 0 & 2 - \lambda 
                \end{bmatrix}
                \\
                                &\implies det(A - \lambda I_3) = (2-\lambda)(1-\lambda)^2 
                                \\
                                &\implies \lambda_1 = 2, \quad \lambda_2 = \lambda_3 = 1
                                \\
                                \\
                    A - I_3 &= \begin{bmatrix}
                        0 & 1 & 0 \\
                        0 & 0 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix} \implies \underline{v_1} = \begin{bmatrix}
                        1 \\
                        0 \\
                        0
                    \end{bmatrix}
            \end{align*}
            Dimensjonen til egenrommet til $\lambda = 1 = 1$
        \end{Example}
        
        \begin{Theorem}{}{}
            Egenrommet til $\lambda$ har dimensjon $\ge$ 1 og geometrisk multiplisitet $\le$ algrebraisk multiplsitet
        \end{Theorem}

        \begin{Theorem}{}{}
            Egenverdiene til $A$ utgjør en basis hvis og bare hvis algebraisk multiplisitet $=$ geometrisk multiplisitet for alle egenverdier.
        \end{Theorem}


        \section{Forelesning 21.10.19}
        \subsection{Oppsummering kap 10}%
        \label{sub:oppsummering_kap_10}
        
        $A$ $n$x$n$ matrise, $\underline{v} \neq 0$ slik at $A\underline{v} = \lambda\underline{v}$; Egenverdi: $\lambda$ og egenvektor $\underline{v}$

        Finner $\lambda$ ved å løse $det(A - \lambda I_{n}) = 0$

        Finner $\underline{v}$ til $\lambda$ ved å finne $\underline{v} \in Null(A - \lambda I-n)$ 

        Egenrommet til $\lambda$ er $Null(A - \lambda I_{n})$

        Finnes alltid $n$ egenverdier til $\lambda$

        Finnes minst en $\underline{v}$ per $\lambda$

        Dersom algebraisk multiplisitet = geometrisk multiplisitet, for alle $\lambda \implies \underline{v_1}, \underline{v_2}, \dots ,\underline{v_{n}}$ basis. 

        \subsection{Diagonalisering}%
        \label{sub:diagonalisering}
        
        \begin{Definition}{}{}
            $A$ $n$x$n$ matrise er diagonaliserbar dersom det finnes en diagonal matrise $D$ ig invertibel matrise $P$ slik at $A = PDP^{-1}$ 
        \end{Definition}

        \section{Diagonalisering}
        Vi har $D = \begin{bmatrix}
            3 & 0 \\
            0 & -5
        \end{bmatrix}$. Hva er $D^5$?

        $D^5 = D \cdot D \cdot D \cdot D \cdot D$. 

        $D^2 = \begin{bmatrix}
            3^2 & 0 \\
            0 & (-5)^2
        \end{bmatrix} \implies D^5 = \begin{bmatrix}
            3^5 & 0 \\
            0 & (-5)^5
        \end{bmatrix} = \begin{bmatrix}
            243 & 0 \\
            0 & -3125
        \end{bmatrix}$
        
        
        Hva om vi har $A = \begin{bmatrix}
            1 & 3 \\
            4 & -3
        \end{bmatrix}$, hva blir $A^5$? Det er vanskelig. Vi vil skrive $A = PDP^{-1}$. $A^2 = (PDP^{-1})(PDP^{-1}) = PD^2P^{-1}$ som vil si vi kan skrive $A^5 = PD^5P^{-1}$. 

        Fra tidligere eksempel (tydeligvis) har vi:
        $\lambda_1 = 3, \quad \lambda_2 = -5, \quad \underline{v_1} = \begin{bmatrix}
            3 \\
            2
        \end{bmatrix}, \quad \underline{v_2} = \begin{bmatrix}
            1 \\
            -2
        \end{bmatrix}$.

        Vi hadde også systemene: $A\underline{v_1} = \lambda\underline{v_1}, \quad A\underline{v_2} = \lambda\underline{v_2}$. Vil skrive disse som et system.

        $P = \begin{bmatrix}
            \underline{v_1} & \underline{v_2}
        \end{bmatrix} = \begin{bmatrix}
            3 & 1 \\
            2 & -2
        \end{bmatrix} \implies P^{-1} = \frac{1}{8} \begin{bmatrix}
            2 & 1 \\
            2 & -3
        \end{bmatrix}$

        Kan skrive $A\underline{v_1} = \lambda_1\underline{v_1} + 0\underline{v_2}, \quad A\underline{v_2} = \lambda\underline{v_2}+0\underline{v_1}$

        $D = \begin{bmatrix}
            3 & 0 \\
            0 & -5
        \end{bmatrix}$
        
        \begin{Definition}{}{}
            En $n$x$n$ matrise $A$ er diagonaliserbar dersom det eksisterer en invertibel $P$ og diagonal $D$ slik at $A = PDP^{-1}$
        \end{Definition}
        
        \begin{Example}{}{}
            $A = \begin{bmatrix}
                1 & 3 \\
                4 & -3
            \end{bmatrix}$, $\lambda_1 = 3$, $\lambda_2 = -5$, $\underline{v_1} = \begin{bmatrix}
                3 \\
                3
            \end{bmatrix}$, $\underline{v_2} = \begin{bmatrix}
                1 \\
                -2
            \end{bmatrix}$.

            $A\underline{v_1} = \lambda_1\underline{v_1} + 0\cdot \underline{v_2}$

            $A\underline{v_2} = 0\cdot \underline{v_2} 0 \lambda_2 \underline{v_2}$
            \\

            Dette vil si:
            \\
            \\
            $A[\underline{v_1} \underline{v_2}] = \begin{bmatrix}
                \underline{v_1} & \underline{v_2}
            \end{bmatrix}\begin{bmatrix}
                \lambda_1 & 0 \\
                0 & \lambda_2
            \end{bmatrix}$

            $\implies A = PDP^{-1}$
        \end{Example}

        \begin{Theorem}{}{}
            $n$x$n$ matrise $A$ er diagonaliserbar $\iff$ $A$ har $n$ linerært uavhengige egenvektorer.
        \end{Theorem}

        \begin{Proof}{Theorem 9.1}{}
            Anta $A$ har $n$ lineært uavhengige egenvektorer $\underline{v_1}, \dots ,\underline{v_{n}}$. Da vet vi at vi har $n$ egenverdier (trenger ikke være unike) slik at $A\underline{v_{i}} = \lambda_{i}\underline{v_{i}} \forall i$

            Da følger det at $\begin{bmatrix}
                A\underline{v_1} & A\underline{v_2} & A\underline{v_{n}}
            \end{bmatrix} = \begin{bmatrix}
                \lambda\underline{v_1} & \lambda\underline{v_2} & \lambda\underline{v_{n}}
            \end{bmatrix}$ som er det samme som $A\begin{bmatrix}
                \underline{v_1} & \underline{v_2} & \underline{v_{n}}\end{bmatrix} = \begin{bmatrix}
                    \underline{v_1} & \underline{v_2} & \underline{v_{n}}
                \end{bmatrix}\begin{bmatrix}
                    \lambda_1 & 0 & \dots & 0 \\
                    0 & \lambda_2 & \dots & 0 \\
                    0 & 0 & \dots & \lambda_{n}
                \end{bmatrix}$

                $\implies A = PDP^{-1} \implies A$ er diagonaliserbar.
                \\
                \\
                Anta nå $A$ er diagonaliserbar. Det vil si det finnes $D$ og $P$ slik at $A = PDP^{-1}$. Da kan vi gange med $P$ på begge sider: $AP = PD$
                \\
                $\implies \begin{bmatrix}
                    A\underline{k_1} & A\underline{k_2} & \dots & \underline{kn}
                \end{bmatrix} = \begin{bmatrix}
                    a_1\underline{k_1} & a_2\underline{k_2} & \dots & a_{n}\underline{kn}
                \end{bmatrix}$
                \\
                \\
                $\implies A\underline{k_{i}} = a_{i}\underline{k_{i}} \forall i \implies a_{i}$ egenverdi for $\underline{k_{i}}$ som er egenvektor for $A$. 
                \\
                \\
                $P$ invertibel $\implies \underline{k_1}, \underline{k_2}, \dots ,\underline{k_{n}}$ er lineært uavhengige $\implies \underline{k_1}, \dots ,\underline{k_{n}}$ lineært uavhengige egenvektorer til $A$. 
        \end{Proof}

        \begin{Theorem}{}{}
            Dersom $n$x$n$ matrisen $A$ har $n$ distinkte egenverdier, så er $A$ diagonaliserbar. 
        \end{Theorem}

        \begin{Theorem}{}{}
            $A$ $n$x$n$ matrise. Da er $A$ diagonaliserbar hvis og bare hvis algebraisk multiplisitet = geometrisk multiplisitet for alle $\lambda$
        \end{Theorem}

        Merk: Reell matrise $A$ kan ha kompleks $P$ og $D$ slik at $A = PDP^{-1}$

        \begin{Example}{}{}
            $A = \begin{bmatrix}
                -8 & 0 & 6 \\
                12 & 4 & -6 \\
                -20 & 0 & 14
            \end{bmatrix}$

            $A$ har egenverdier $\lambda_1 = 2$, $\lambda_2 = 4$ og $\lambda_3 = 4$, med egenvetorer $\underline{v_1} = \begin{bmatrix}
                3 \\
                -3 \\
                5
            \end{bmatrix}$, til $\lambda = 2$, $\underline{v_2} = \begin{bmatrix}
                0 \\
                1 \\
                0
            \end{bmatrix}$ og $\underline{v_3} = \begin{bmatrix}
                1 \\
                0 \\
                2
            \end{bmatrix}$ til $\lambda = 4$.

            Kan sette opp 
            \\
            $D = \begin{bmatrix}
                2 & 0 & 0 \\
                0 & 4 & 0 \\
                0 & 0 & 4
            \end{bmatrix}$ hvor tallene som ikke er $0$ er egenverdiene.
            \\
            \\
            \\
            $P = \begin{bmatrix}
                3 & 0 & 1 \\
                -3 & 1 & 0 \\
                5 & 0 & 2
            \end{bmatrix}$ hvor kolonnene er egenvektorene.
            \\
            \\
            \\
            Da er $A = PDP^{-1}$. 
        \end{Example}
        
        \begin{Example}{}{}
            $A = \begin{bmatrix}
                0 & -1 \\
                1 & 0
            \end{bmatrix}$. Har komplekse egenverdier $\lambda_1 = i$, $\lambda_2 = -1$. Egenvektorer: $\underline{v_1} = \begin{bmatrix}
                1 \\
                -i
            \end{bmatrix}$ og $\underline{v_2} = \begin{bmatrix}
                1 \\
                i
            \end{bmatrix}$. 
            \\
            \\
            \\
            Egenverdiene gir oss en diagonalmatrise $D = \begin{bmatrix}
                i & 0 \\
                0 & -1
            \end{bmatrix}$ og egevektorene gir oss $P = \begin{bmatrix}
                1 & 1 \\
                -i & i
            \end{bmatrix} \implies P^{-1} = \frac{1}{2i} \begin{bmatrix}
                i & -1 \\
                i & 1
            \end{bmatrix} = \frac{-2i}{4} = \frac{1}{2} \begin{bmatrix}
                1 & i \\
                1 & -i
            \end{bmatrix}$ 
            \\
            \\
            \\
            \begin{align*}
                \begin{bmatrix}
                    1 & 1 \\
                    -i & i
                \end{bmatrix} \cdot \begin{bmatrix}
                    i & 0 \\
                    0 & -i
                \end{bmatrix} &= \begin{bmatrix}
                i & -i \\
                1 & 1
                \end{bmatrix}
                \\
                \\
               \frac{1}{2} \begin{bmatrix}
                   i & -i \\
                   1 & 1
               \end{bmatrix} \cdot \begin{bmatrix}
                   1 & i \\
                   1 & -i
               \end{bmatrix} &= \frac{1}{2} \begin{bmatrix}
               0 & -2 \\
               2 & 0
               \end{bmatrix} = \begin{bmatrix}
               0 & -1 \\
               1 & 0
               \end{bmatrix} = A
            \end{align*}
        \end{Example}

        \begin{Example}{}{}
            $A = \begin{bmatrix}
                1 & 1 \\
                0 & 1
            \end{bmatrix}$

            Finner egenverdier og egenvektorer:
            \begin{align*}
                A - \lambda I_{2} &= \begin{bmatrix}
                    1 - \lambda & 1 \\
                    0 & 1 - \lambda
                \end{bmatrix}
                \\
                                  &\implies det(A - \lambda I_1) = (1-\lambda)^2 = 0
                                  \\
                                  &\implies \lambda_1 = 1, \quad \lambda_2 = 1
                                  \\
                                  \\
                    A - I_2 &= \begin{bmatrix}
                        0 & 1 \\
                        0 & 0
                    \end{bmatrix} \implies \underline{v} = \begin{bmatrix}
                        1 \\
                        0
                    \end{bmatrix}
            \end{align*}
            Algebraisk multiplisitet er 2, mens geometisk er 1. $A$ er ikke diagonaliserbar.
        \end{Example}


        Har en matrise $A = \begin{bmatrix}
            a & -b \\
            b & a
        \end{bmatrix}$ hvor $a, b \in \mathbb{R}$ og $b \neq 0$.

        $det(A - \lambda I_{2}) = (a-\lambda)^2 + b^2 = 0 \implies \lambda_1 = a+ib$, $\lambda_2 = a - ib$.

        Ser på $A - (a+ib)I_{2} = \begin{bmatrix}
            -ib & -b \\
            b & -ib
        \end{bmatrix} \sim \begin{bmatrix}
            -ib & -b \\
            0 & 0
        \end{bmatrix} \implies \underline{v_1} = \begin{bmatrix}
            1 \\
            -i
        \end{bmatrix} \implies \underline{v_2} = \begin{bmatrix}
            1 \\
            i
        \end{bmatrix}$.

        $r = |\lambda| = \sqrt{a^2 + b^2}$

        $A = \begin{bmatrix}
            r & 0 \\
            0 & r
        \end{bmatrix} \begin{bmatrix}
            \cos{\theta} & -\sin{\theta} \\
            \sin{\theta} & \cos{\theta}
        \end{bmatrix}$

        \begin{Theorem}{}{}
            $A$ reell $2$x$2$ matrise med kompleks egenverdi $\lambda = a-ib$ (eller $\lambda = a+ib$) hvor $b>0 \in \mathbb{R}$. 
            
            La $\underline{v} \in \mathbb{C}^2$ være egenvektor til $\lambda$ da kan vi skrive $A = PCP^{-1}$, hvor $P = \begin{bmatrix}
                Re\underline{v} & Im\underline{v}
            \end{bmatrix}$ og $C = \begin{bmatrix}
                a & -b \\
                b & a
            \end{bmatrix}$ 
        \end{Theorem}
        
        \begin{Example}{}{}
            \begin{align*}
                A &= \begin{bmatrix}
                    1 & -2 \\
                    1 & 3
                \end{bmatrix}
                \\
                    det(A - \lambda I_{2}) &= (1-\lambda)(3-\lambda) + 2
                    \\
                                           &= \lambda^2 - 4\lambda + 5 = 0
                                           \\
                                           &\implies \lambda = \left[ 2 - i, \  2 + i\right]
                                           \\
                                           \\
                    A - (2-i)I_2 &= \begin{bmatrix}
                        -i+i & -2 \\
                        1 & 1+i
                    \end{bmatrix}
                    \\
                                 &\sim \begin{bmatrix}
                                     1-i & 2 \\
                                     0 & 0
                                 \end{bmatrix}
                                 \\
                                 &\sim \begin{bmatrix}
                                     0 & 0 \\
                                     1 & 1+i
                                 \end{bmatrix}
                                 \\
                                 &\implies \underline{v} = \begin{bmatrix}
                                     1 + i \\
                                     -1
                                 \end{bmatrix}, \quad Re(\underline{v}) = \begin{bmatrix}
                                     1 \\
                                     -1
                                 \end{bmatrix}, \quad Im(\underline{v}) = \begin{bmatrix}
                                     1 \\
                                     0
                                 \end{bmatrix}
                                 \\
                                 &\implies P = \begin{bmatrix}
                                     1 & 1 \\
                                     -1 & 0
                                 \end{bmatrix}, \quad C = \begin{bmatrix}
                                     2 & -1 \\
                                     1 & 2
                                 \end{bmatrix}
            \end{align*}
        \end{Example}
        
        \section{Forelesning 24.10.19}
        
        $T_{\theta} = \begin{bmatrix}
            \cos{\theta} & -\sin{\theta} \\
            \sin{\theta} & \cos{\theta}
        \end{bmatrix}$ \\
        \\
        $T_{\theta} \cdot T_{\theta} = T_{2\theta}$

        \begin{Example}{}{}
            Lol, dette var samme eksempel som det forrige, jaja, skirver det opp på nytt. :)
            $$
                A = \begin{bmatrix}
                    1 & -2 \\
                    1 & 3
                \end{bmatrix}
            $$

            Skriv $A$ som $PCP^-1$
            
            Strategi:
            \begin{enumerate}
                \item Finn egenverdiene $\lambda$
                \item Finn egenvektorene $\underline{v}$
                \item finn $r$ og $\theta$
                \item Sett opp $A = PCP^-1$
            \end{enumerate}

            \begin{align*}
                det(A - \lambda I_{2}) &= (1-\lambda)(3-\lambda)+2 \\
                                       &= \lambda^2 - 4\lambda + 5 \\
                                       &\implies \lambda = \left[ 2 - i, \  2 + i\right]
            \end{align*}
            Vi har nå $a=2$ og $b=1$
            
            Finner egenvekorer:
            \begin{align*}
                A - (2-i)I_{2} &= \begin{bmatrix}
                    1 & -2 \\
                    1 & 3
                \end{bmatrix} - \begin{bmatrix}
                    (2-i) & 0 \\
                    0 & (2-i)
                \end{bmatrix} \\
                               &= \begin{bmatrix}
                                   -1 + i & -2 \\
                                   1 & 1 + i
                               \end{bmatrix}
                               \\
                               &\sim \begin{bmatrix}
                                   0 & 0 \\
                                   1 & 1+i
                               \end{bmatrix}
                               \\
                               &\implies \underline{v} = \begin{bmatrix}
                                   1 + i \\
                                   -1
                               \end{bmatrix}
                               \\
                               \\
                               &\implies Re\underline{v} = \begin{bmatrix}
                                   1 \\
                                   -1
                               \end{bmatrix}, \quad Im\underline{v} = \begin{bmatrix}
                                   1 \\
                                   0
                               \end{bmatrix}
            \end{align*}

            Finner $r$ og $\theta$
            \begin{align*}
                r &= \sqrt{2^2 + 1^2} = \sqrt{5}
                \\
                \theta &= \arccos\left(\frac{2}{\sqrt{5}}\right) = 26.5\text{grader}
                \\
                (\cos{\theta} = \frac{2}{\sqrt{5}}, \sin{\theta} = \frac{1}{\sqrt{5}})
            \end{align*}
            
            Finner nå $A$?

            \begin{align*}
                A &= \begin{bmatrix}
                    1 & 1 \\
                    -1 & 0
                \end{bmatrix}\begin{bmatrix}
                    2 & -1 \\
                    1 & 2
                \end{bmatrix}\begin{bmatrix}
                    0 & -1 \\
                    1 & 1
                \end{bmatrix}
                \\
                \\
                C = \begin{bmatrix}
                    \sqrt{5} & 0 \\
                    0 & \sqrt{5}
                \end{bmatrix}\begin{bmatrix}
                    \frac{2}{\sqrt{5}} & -\frac{1}{\sqrt{5}} \\
                    \frac{1}{\sqrt{5}} & \frac{2}{\sqrt{5}}
                \end{bmatrix}
            \end{align*}
        \end{Example}

        $A\underline{v} = PCP^-1\underline{v} = PC\underline{u} = PDT_{\theta}\underline{u} = PD\underline{u}_{\theta} = P\underline{u}_{\theta} = A\underline{v}$

        \section{Symmertiske matriser}
        
        \begin{Definition}{}{}
            En $n$x$n$ matrise $A$ er symmetrisk dersom $A = A^T$
        \end{Definition}

        \begin{Example}{Symmetrisk $3x3$}{}
            $$
                A = \begin{bmatrix}
                    1 & -5 & 7 \\
                    -5 & 2 & -13 \\
                    7 & -13 & 3
                \end{bmatrix} = A^T
            $$
            Er symmetrisk. Hvis vi speiler elementene om diagonalen, får vi samme matrise. 
        \end{Example}
        
        \begin{Example}{Symmetrisk $2x2$}{}
            $$
                A = \begin{bmatrix}
                    a & b \\
                    b & c
                \end{bmatrix}
            $$

            \begin{align*}
                A - \lambda I_2 &= \begin{bmatrix}
                    a-\lambda & b \\
                    b & c-\lambda
                \end{bmatrix} \\
                    det(A - \lambda I_2) &= (a-\lambda)(c-\lambda) - b^2 = 0 \\
                                         &= \lambda^2 -(a+c)\lambda - ac-b^2 \\
                    \implies \lambda = \frac{a+c \pm \sqrt{(a+c)^2 - 4(ac-b^2)}}{2}
                \end{align*}

                Er utrykket under roten større enn 0?
                \begin{align*}
                    (a+c)^2 -4ac + 4b^2 &= a^2 +2ac + c^2 - 4ac + 4b^2 \\
                                        &= (a-c)^2 + 4b_4 \; \underline{>} \; 0
                \end{align*}
                
                Dersom $(a-c)^2 + 4b^2 = 0$ får vi en dobbelrot. 

                Dersom $(a-c)^2 + 4b^2 > 0$ for vi to unike egenverdier. 
                

                Hvis vi har dobbelrot får vi:
                \begin{align*}
                    A = P\begin{bmatrix}
                        \lambda & 0 \\
                        0 & \lambda
                    \end{bmatrix}P^-1 = \lambda PI_2P^-1 \implies A = \begin{bmatrix}
                        \lambda & 0 \\
                        0 & \lambda
                    \end{bmatrix}
                \end{align*}
                
        \end{Example}
        \begin{Theorem}{}{}
            $A$ symmetrisk $n$x$n$ matrise. Da har $A$ $n$ reelle egenverdier og $A$ er diagonaliserbar.
        \end{Theorem}

        \begin{Example}{}{}
            La

            $$
                A = \begin{bmatrix}
                    1 & 2 & 2 \\
                    2 & 6 & 2 \\
                    2 & 2 & 6
                \end{bmatrix}
            $$
            være en symmetrisk matrise, vet ikke helt hva oppgaven egt er, men skriver ned det han gjør.
            \begin{align*}
                det(A - \lambda I_3) &= (1-\lambda)(6-\lambda)^2+16 - (4(6-\lambda(6-\lambda))) + (1-\lambda) \\
                                     &= \lambda^3 - 13\lambda^2 + 36\lambda \\
                                     &\implies \lambda = \left[ 0, \  4, \  9\right]\\
                                     &\underline{v_1} = \begin{bmatrix}
                                         -4 \\
                                         1 \\
                                         1
                                     \end{bmatrix}, \quad \underline{v_2} = \begin{bmatrix}
                                         1 \\
                                         2 \\
                                         2
                                     \end{bmatrix} \quad \underline{v_3} = \begin{bmatrix}
                                         0 \\
                                         1 \\
                                         -1
                                     \end{bmatrix} 
            \end{align*}
            Vi kan nå skrive $A = PDP^-1$ der \begin{align*}
            D &= \begin{bmatrix}
                    0 & 0 & 0 \\
                    0 & 4 & 0 \\
                    0 & 0 & 9
                \end{bmatrix} \\
                P &= \begin{bmatrix}
                    -4 & 0 & 1 \\
                    1 & 1 & 2 \\
                    1 & -1 & 2
                \end{bmatrix}    
            \end{align*}

Egenvektorene er også ortogonale! Dette er sant for alle symmetriske matriser (der vi har distinkte egenvektorer)
        \end{Example}
        
        \begin{Theorem}{}{}
            $A$ symmetrisk $n$x$n$ matrise. Da er egenvektorene tilhørende forskjellige egenverdier ortogonale. 
        \end{Theorem}

        \begin{Definition}{}{}
            En $n$x$n$ matrise $A$ kalles \textit{hermitsk} dersom $A = A^*$ (kompleks konjugert og transponert)
        \end{Definition}

        \section{Markov kjeder}
        
        \begin{Example}{}{}
            Anta barna i barnehagen holder emd enten Manchester United, Liverpool eller Arsenal. Barna er såpass små at de ikke har lært seg at det ikke er lov til å bytte favorittklubb. 

            Sannsynligheten for at et barn fortsatt heier på MU etter et år er $50\%$. Sannsynligheten for at de bytter til Liverpool er $30\%$, $20\%$ til Liverpool.
            Hvis de heier på Liverpool et år, er sannsynligheten $20\%$ for at de bytter til MU, $80\%$ for at de fortsetter med Liverpool og $0\%$ for at de bytter til Arsenal.

            Hvis de heier på Arsenal et år er det $30\%$ for at de bytter til MU, $30\%$ til L og $40\%$ til L. Kan sette opp som en tabell:

            \begin{tabular}{c|c|c|c|c}
                \hline
                fra  & MU & L & A & Til 
                \\
                \hline
                     & 50 & 20 & 30 & MU \\
                     \hline
                     & 30 & 80 & 30 & L \\
                     \hline
                     & 20 & 0 & 40 & A
            \end{tabular}

            Kan også sette opp som matrise:
            \begin{align*}
                M = \begin{bmatrix}
                    0.5 & 0.2 & 0.3 \\
                    0.3 & 0.8 & 0.3 \\
                    0.2 & 0 & 0.4
                \end{bmatrix}
            \end{align*}
                    
            Får en startvektor $\begin{bmatrix}
                0.5 \\
                0.3 \\
                0.2
            \end{bmatrix}$

            
            Hva er fordelingen etter $1$ år? Det blir startvektor ganger matrisen $M$ = $\begin{bmatrix}
                0.37 \\
                0.45 \\
                0.18
            \end{bmatrix}$


            Hva med etter 2 år? Det blir $M^2 \cdot x_0 = \begin{bmatrix}
                0.329 \\
                0.525 \\
                0.146
            \end{bmatrix}$


            Etterhvert vildu kommer til en ''Likevektsstilling'' som ser ut som $q = \begin{bmatrix}
                0.3 \\
                0.6 \\
                0.1
            \end{bmatrix}$, slik at $Mq = q$. Da er $q$ egenvektor for $M$ med egenverdi $\lambda = 1$
        \end{Example}
        
        \begin{Definition}{}{}
            En vektor $\underline{x}$ hvor alle koordinatene er positive reelle tall og koordinatene summerer til $1$ (elementene i kolonnene), kalles en sannsynlighetsvektor. 
        \end{Definition}
        
        \begin{Definition}{}{}
            En matrise $M$ hvor alle kolonnene er en sannsynlihetsvektor, kalles $M$ en stokastisk matrise. 
        \end{Definition}

        \begin{Definition}{}{}
            Gitt $M$ og $x_0$, da kalles følgen $x_0, x_1, x_2, \dots$ hvor $x_{i} + 1 = M \cdot x_{i}$ en Markov kjede. 
        \end{Definition}

        \begin{Theorem}{}{}
            En stokastisk matrise har alltid egenverdi $\lambda = 1$
            \begin{Proof}{}{}
                \begin{align*}
                    M = \begin{bmatrix}
                        M_{11} & M_{12} & \dots & M_{1n}
                        \\
                        M_{21} & M_{22} & \dots & M_{2n}
                        \\
                        M_{31} & M_{32} & \dots & M_{3n}
                    \end{bmatrix}
                \end{align*}
                Hver kolonne kummerer til $1$.
                \begin{align*}
                    M^T = \begin{bmatrix}
                        M_{11} & M_{21} & \dots & M_{31} \\
                        M_{12} & M_{22} & \dots & M_{32} \\
                        M_{31} & M_{23} & \dots & M_{33}
                    \end{bmatrix}
                \end{align*}
                $M^T$ har egenverdi $\lambda = 1$ og egenvektor $\begin{bmatrix}
                    1 \\
                    1 \\
                    ... \\
                    1
                \end{bmatrix}$
                Vi får da $M^{T}\underline{v} = a \cdot \begin{bmatrix}
                    1 \\
                    1 \\
                    ... \\
                    1
                \end{bmatrix}$
                
                \begin{align*}
                    det(M^{T} - \lambda I_{n}) &= det(M^{T} - \lambda I^{T}_{n})\\
                                               &= det((M - \lambda I_{n})^T)
                                               \\
                                               \text{Generelt: det$A$ =det$A^T$}
                                               \\
                                               &= det(M - \lambda I_{n})
                                               \\
                    \implies \lambda &= 1 \: \text{egenverdi for } M
                \end{align*}
            \end{Proof}
           \end{Theorem}
            \begin{Definition}{}{}
                Dersom $\underline{q}$ er en egenvektor til $\lambda = 1$ for stokastisk matrise $M$, og $\underline{q}$ er sannsynlighetsvektor, så kalles $\underline{q}$ for en \textit{likevektsvektor}

                Merk:

                Gitt vektor $\underline{x} = \begin{bmatrix}
                    x_1 \\
                    x_2 \\
                    ... \\
                    x_{n}
                \end{bmatrix}$, så er \begin{align*}
                    \frac{1}{\sum_{i=1}^{n}x_{i}} \underline{x}
                \end{align*}
                en vektor med koordinatsum lik $1$
            \end{Definition} 

            \begin{Example}{}{}
                Fortsetter med fotballag-eksemepelt fra over.

                Har \begin{align*}
                    M = \begin{bmatrix}
                        0.5 & 0.2 & 0.3 \\
                        0.3 & 0.8 & 0.3 \\
                        0.2 & 0 & 0.4
                    \end{bmatrix}
                \end{align*}
                Kan finne \begin{align*}
                    M - I &= \begin{bmatrix}
                        0 & 0 & 0 \\
                        3 & -2 & 3 \\
                        2 & 0 & - 6
                    \end{bmatrix} \: \text{Ganget med 10 og gausset.}
                    \\
                          &\sim \begin{bmatrix}
                              0 & 0 & 0 \\
                              1 & -2 & 9 \\
                              1 & 0 & -3
                          \end{bmatrix}
                          \\
                          &\sim \begin{bmatrix}
                              0 & 0 & 0 \\
                              0 & 1 & -6 \\
                              1 & 0 & -3
                          \end{bmatrix}
                          \\
                        \implies \underline{v} &= \begin{bmatrix}
                            3 \\
                            6 \\
                            1
                        \end{bmatrix} \implies q
                \end{align*}
            \end{Example}

            \begin{Definition}{}{}
                En stokastisk matrise $M$ kalles \textit{regulær}, dersom det finnes en $k \geq 1$ slik at alle elementer i $M^k > 0$
            \end{Definition}
            \begin{Theorem}{}{}
                $M$ regulær stokastisk matrise. Da har $M$ unik likevektor $\underline{q}$ og uansett startvektor $x_0$, så vil Markovkjeden konvergere mot $\underline{q}$.
            \end{Theorem}

            \section{System av differensiallikninger}
            (Nome er tilbake, ikke sikkert det blir bra notater videre.)

            Først: Litt om vektorfunkjsoner:
            \subsection{Vektorfunksjoner}%
            \label{sub:vektorfunksjoner}
            
            \begin{align*}
                \underline{x}(t) &= \begin{bmatrix}
                    x_1(t) \\
                    x_2(t) \\
                    .. \\
                    x_{n}(t)
                \end{bmatrix}, \; \mathbb{R} \rightarrow \mathbb{R}^{n} (\mathbb{C}^{n})
                \\
                    \underline{x}'(t) &= \begin{bmatrix}
                        x_1'(t) \\
                        x_2'(t) \\
                        ... \\
                        x_{n}'(t)
                    \end{bmatrix}
            \end{align*}

            $\underline{x}'(t)$ gir tangentvektoren til posisjonen til fluen.

            $||\underline{x}'(t)||$ kalles ''banefart''.
            
            \begin{Example}{}{}
                $\underline{x}(t) = e^{it} = \cos{t} + i \sin{t}$

                Her kan vi tenkte på $t$ som tiden en flue bruker på å flytte seg, og output er hvor langt den har bevegd seg. 
            \end{Example}

            Dersom $\underline{x}(t)$ er deriverbar, og $||\underline{x}'(t) \neq 0||$ er $\underline{x}(t)$ glatt. For eksempel er $\underline{x}'(t) = \begin{bmatrix}
                t^2 \\
                t^3
            \end{bmatrix}$ \textit{ikke} glatt. 
            
            $\underline{x}(t), \underline{x_1}(t), \dots ,\underline{x_{n}}(t)$ er lineært uavhengig, dersom de er lineært uavhengige for alle $t$.

            \subsection{Systemer av differensiallikninger}%
            \label{sub:systemer_av_differensiallikninger}
            
            \begin{Example}{}{}
                $\underline{x}'(t) = 2\underline{x}(t) + \underline{y}(t)$
                    \\
                    \\
                $\underline{y}'(t) = \underline{x}(t) + 2\underline{y}(t)$

                Liker å skrive systemet som:
                \begin{align*}
                    \begin{bmatrix}
                    \underline{x}'(t) \\
                        \underline{y}'(t)
                    \end{bmatrix} &= \begin{bmatrix}
                    2 & 1 \\
                    1 & 2
                    \end{bmatrix}
                \end{align*}
                Oppgave: Finn en $\underline{y}$
                \begin{align*}
                    \underline{y}' &= \begin{bmatrix}
                        2 & 1 \\
                        1 & 2
                    \end{bmatrix}\underline{y}
                    \\
                        \implies \underline{y_1} &= 3e^{3t}\begin{bmatrix}
                                       1 \\
                                       1
                                   \end{bmatrix}
                \end{align*}
                
                Fun fact: Derosm $\underline{x}$ er egenvektor til $A$, med egenverdi $\lambda$ løser $\underline{y}(t) = e^{\lambda t}\underline{x}$ systemet $\underline{y}' = A\underline{y}$
            \end{Example}

            \begin{Theorem}{}{}
                Dersom $A$ er diagonaliserbar, finnes $n$ lineært uavhengige løsninger. 

                (Dette fordi $A$ diagonaliserbar $\implies n$ uanhengige egenvektorer)
            \end{Theorem}

            \begin{Theorem}{}{}
                Dersom $A$ ikke er diagonaliserbar, finnes det $n$ lineært uavhengige løsninger. 
            \end{Theorem}

            Disse to teoremene sier i praksis at det alltid er $n$ lineært uanvhengige løsninger, men hvis $A$ ikke er diagonaliserbar, er de for vanskelige for oss å finne. Derfor - på eksamen, hvis vi skal finne løsning av et system, kan vi anta $A$ er diagonaliserbar. 

            \begin{Example}{}{}
                $$\underline{y}'(t) = \begin{bmatrix}
                    2 & 1 \\
                    1 & 2
                \end{bmatrix} \underline{y}(t)$$ 

                Finn ''den andre'' løsningen.

                Egenverdier: $\lambda_1 = 1$, $\lambda_2 = 3$. 

                Egenvektorer: $\begin{bmatrix}
                    1 \\
                    -1
                \end{bmatrix}$ (for $\lambda_{1}$).

                Vi har dermed:
                \begin{align*}
                    \underline{y_2}(t) = e^{t}\begin{bmatrix}
                        1 \\
                        -1
                    \end{bmatrix}
                \end{align*}
                Denne løsningen pluss den forrige løsningen er også en løsning.

                En generell løsning for systemet er:
                \begin{align*}
                    \underline{y}(t) = c_1e^{\lambda_1 t}\underline{x_1} + c_2e^{\lambda_2 t}\underline{x_2} + \dots + c_{n}e^{\lambda_{n} t}\underline{x_{n}}
                \end{align*}
            \end{Example}

            \subsection{Initialverdiproblem}%
            \label{sub:initialverdiproblem}
            
            Vi har $\underline{y}' = A\underline{y}$ og en $\underline{y}(t_0)$ ''der kalaset skal starte'' og har en $\underline{y_0}$ som $\underline{y}(t_0)$ skal bli.

            Hvis vi har $\underline{y}' = \begin{bmatrix}
                2 & 1 \\
                1 & 2
            \end{bmatrix}\underline{y}$ og $\underline{y}(0) = \begin{bmatrix}
                2 \\
                0
            \end{bmatrix}$

            Setter inn generell løsning:
            \begin{align*}
                \underline{y}(0) &= c_1\begin{bmatrix}
                    1 \\
                    1
                \end{bmatrix} + c_2\begin{bmatrix}
                    1 \\
                    -1
                \end{bmatrix} = \begin{bmatrix}
                    2 \\
                    0
                \end{bmatrix}
                \\
                    \implies c_1 &= c_2 = 1
                    \\
                    \implies \underline{y}(t) &= e^{3t}\begin{bmatrix}
                        1 \\
                        1
                    \end{bmatrix} + e^{t}\begin{bmatrix}
                        1 \\
                        -1
                    \end{bmatrix}
            \end{align*}

            \begin{Theorem}{}{}
                $\underline{y}'$, $\underline{y}(t_0) = \underline{y_0}$ har alltid en entydig løsning.
            \end{Theorem}
            \begin{Example}{}{}
                $$\underline{y}' = \begin{bmatrix}
                    1 & 2 & 2 \\
                    2 & 6 & 2 \\
                    2 & 2 & 6
                \end{bmatrix}\underline{y}$$

                Finn løsningen på problemet.

                Egenverdiene til matrisen er $0, 4$ og $9$. 

                Egenvektor for $\lambda = 0$: $\begin{bmatrix}
                    -4 \\
                    1 \\
                    1
                \end{bmatrix}$

                Egenvektor for $\lambda = 4$: $\begin{bmatrix}
                    0 \\
                    1 \\
                    -1
                \end{bmatrix}$

                Egenvektor for $\lambda = 9$: $\begin{bmatrix}
                    1 \\
                    2 \\
                    2
                \end{bmatrix}$

                Det vil da si den generelle løsningen er:
                \begin{align*}
                    \underline{y}(t) &= c_1e^{0}\begin{bmatrix}
                       -4 \\
                       1 \\
                       1
                   \end{bmatrix} + c_2e^{4t}\begin{bmatrix}
                       0 \\
                       1 \\
                       -1
                   \end{bmatrix} + c_2e^{9t}\begin{bmatrix}
                       1 \\
                       2 \\
                       2
                   \end{bmatrix}
                \end{align*}
                Det første leddet kalles en likevektsløsning.
            \end{Example}

            \subsection{Fasediagram}%
            \label{sub:fasediagram}
            
            (Kun $\mathbb{R}^2$)


            Et fasediagram er et koordinatsystem, hvor vi har en scetch av løsningene våre. 

            Har løsningen fra tidligere $\underline{y}(t) = c_1e^{3t}\begin{bmatrix}
                1 \\
                1
            \end{bmatrix} + c_2e^{t}\begin{bmatrix}
                1 \\
                -1
            \end{bmatrix}$

            Steg for å finne fasediagram:
            \begin{enumerate}
                \item Tegn egenrommene
                \item Tegn noen løsningskurver (løs $\underline{y}(0)$ med forskjellige startverdier).
                \item Indiker retning
            \end{enumerate}


            \section{Forelesning 08.11.19}
            \subsection{Litt repitisjon}%
            \label{sub:litt_repitisjon}
            
            $\underline{y'} = A\underline{y}$ Sistem av defflikning

            Generell løsning: $\underline{y}(t) = c'e^{\lambda_1t}x_1 + \dots + c_{n}e^{\lambda_{n}t}c_{n}$ Dersom $A$ er diagonaliserbar. 

            \begin{Example}{}{}
                $y_{1}' = y_2$, $y_{2}' = -y_1 \implies \underline{y}' = \begin{bmatrix}
                    0 & 1 \\
                    -1 & 0
                \end{bmatrix}\underline{y}$

                Step 1: Finn egenverdier og egenvektorer.

                \begin{align*}
                    det(A - \lambda I_2) &= (-\lambda)\cdot (-\lambda) - 1\cdot -1
                    \\
                                         &= \lambda^2 +1
                                         \\
                    \implies \lambda &= \left[ - i, \  i\right]
                \end{align*}
               Egenvektor for $\lambda = i$: $\begin{bmatrix}
                   1 \\
                   i
               \end{bmatrix}$ 

               Egenvektor for $\lambda = -i$: $\begin{bmatrix}
                   1 \\
                   -i
               \end{bmatrix}$
               $\implies \underline{y}(t) = c_1e^{it}\begin{bmatrix}
                   1 \\
                   i
               \end{bmatrix} + c_2e^{-it}\begin{bmatrix}
                   1 \\
                   -i
               \end{bmatrix}$

               Kan vi tegne fasediagram av dette? - nei, det er ikke så lett. Vi er nødt til å gjøre noe lurt. 

               Triks 1: $c_1 = c_2 = \frac{1}{2}$. Da får vi $\underline{y_1}(t) = \frac{1}{2}\cdot e^{it}\begin{bmatrix}
                   1 \\
                   i
               \end{bmatrix} + \frac{1}{2} e^{-it}\begin{bmatrix}
                   1 \\
                   -i
               \end{bmatrix} = \frac{1}{2} \begin{bmatrix}
               e^{it} + e^{-it} \\
               ie^{it} - ie^{-it}
               \end{bmatrix} = \begin{bmatrix}
               \cos{t} \\
               -\sin{t}
               \end{bmatrix}$


               Triks 2: $c_1 = \frac{1}{2i}$, $c_2 = -\frac{1}{2i}$ som gir $\underline{y_2}(t) = \frac{1}{2i} e^{it} - \frac{1}{2i}e^{-it} = \begin{bmatrix}
                   \sin{t} \\
                   \cos{t}
               \end{bmatrix}$

               Da får vi $\underline{y}(t) = d_1\begin{bmatrix}
                   \cos{t} \\
                   -sin{t}
               \end{bmatrix} + d_2 \begin{bmatrix}
               \sin{t} \\
               \cos{t}
               \end{bmatrix} = \begin{bmatrix}
               \cos{t} & \sin{t} \\
                -\sin{t} & \cos{t}
               \end{bmatrix} \cdot \begin{bmatrix}
                   d_1 \\
                   d_2
               \end{bmatrix}$

               Fasegiagrammet blir da masse rundinger med forskjellige radiuser, med origo som sentrum.
            \end{Example}
            
            \begin{Example}{}{}
                $\underline{y'} = \begin{bmatrix}
                        1 & 1 \\
                        -1 & 1
                \end{bmatrix}\underline{y}$

                \begin{align*}
                    det(A - \lambda i_2) &= (1-\lambda)^2 + 1 = \lambda^2 - 2\lambda + 2 = 0
                    \\
                    \implies \lambda &= \left[ 1 - i, \  1 + i\right]
                \end{align*}

                Egenvektor for $\lambda = 1+i: \begin{bmatrix}
                    1 \\
                    i
                \end{bmatrix}$

                Egenvektor for $\lambda = 1-i: \begin{bmatrix}
                    1 \\
                    -i
                \end{bmatrix}$

                Vi får $\underline{y}(t) = c_1 e^{(1+i)t} \begin{bmatrix}
                    1\\
                    i
                \end{bmatrix} + c_2 e^{(1-i)t}\begin{bmatrix}
                    1 \\
                    -i
                \end{bmatrix}$
                
                Dette er det samme som $e^{-t} \cdot \left(c_1 e^{it}\begin{bmatrix}
                    1 \\
                    i
                \end{bmatrix} + c_2e^{-ti}\begin{bmatrix}
                    1 \\
                    -i
                \end{bmatrix}\right)$
                
                Moralen er: Imaginær $\lambda$ lager ellipser som fasediagram. Hvis $Re(\lambda) > 0 $ fpr vi utadgående spiraler. Motsatt andre veien. 

            \end{Example}

            \subsection{En ny type likning}%
            \label{sub:en_ny_type_likning}
            
            $\underline{y}' = A\underline{y} + \underline{f}$, der $\underline{f}$ er en vektorfunksjon. Denne likningen kalles en inhomogen likning.

            $\underline{y}' = A\underline{y}$ kalles en homogen likning. 

            En løsning av $\underline{y}' = A\underline{y}$ kalles en homogen løsning $\underline{y}_{h}$

            En løsning av $\underline{y}' = A\underline{y} + \underline{f}$ som ikke inneholder homogene løsninger, kalles en partikulær løsning. (evt. inhomogen løsning.). Skrives $\underline{y}_{p}$

            Den generelle løsningen av $\underline{y}' = A\underline{y} + \underline{f}$ er $\underline{y} = \underline{y}_{h} + \underline{y}_{p}$

            \begin{Example}{}{}
            $\underline{y}' = \begin{bmatrix}
                2 & 1 \\
                1 & 2
            \end{bmatrix} \underline{y} + \begin{bmatrix}
                1 \\
                1
            \end{bmatrix}$ 

            $\underline{y}_{h} = c_1 e^{3t}\begin{bmatrix}
                1 \\
                1
            \end{bmatrix} + c_2e^{t}\begin{bmatrix}
                1 \\
                -1
            \end{bmatrix}$

            $\underline{y}_{p} = \begin{bmatrix}
                c_1 \\
                c_2
            \end{bmatrix}$ Vi putter inn i likningen:

            $\underline{y}' = \begin{bmatrix}
                2 & 1 \\
                1 & 2
            \end{bmatrix}\begin{bmatrix}
                c_1 \\
                c_2
            \end{bmatrix} + \begin{bmatrix}
                1 \\
                1
            \end{bmatrix} \implies \begin{bmatrix}
                2 & 1 \\
                1 & 2
            \end{bmatrix}\begin{bmatrix}
                c_1 \\
                c_2
            \end{bmatrix} = \begin{bmatrix}
                1 \\
                1
            \end{bmatrix} \implies \begin{bmatrix}
                c_1 \\
                c_2
            \end{bmatrix} = \begin{bmatrix}
                -\frac{1}{3} \\
                -\frac{1}{3}
            \end{bmatrix}$


            Som vil si $\underline{y}(t) = \underline{y}(t)_{h} + \underline{y}(t)_{p} = c_1e^{3t}\begin{bmatrix}
                1 \\
                1
            \end{bmatrix} + c_2e^{t}\begin{bmatrix}
                1 \\
                -1
            \end{bmatrix} - \frac{1}{3}\begin{bmatrix}
                1 \\
                1
            \end{bmatrix}$
            \end{Example}

            Teknikk:
            \subsubsection{Variasjon av parametre}%
            \label{ssub:variasjon_av_parametre}
            
            Dette er en formel basert på følgende idé:

            $\underline{y}_{p} = Y(t) c(t)$, der $Y$ er en matrise $Y(t) = \begin{bmatrix}
                e^{\lambda_1 t}\underline{x_1} & \dots & e^{\lambda_n t}\underline{x_{n}} 
            \end{bmatrix}$

            Vi vet at $det(Y(t)) \neq 0$

            $\underline{y}_{p}' = Y'c + Yc'$ 

            Setter inn i $\underline{y}' = A\underline{y} + \underline{f}$ og får $Y'c + Yc' = AYc + \underline{f}$

            Kan skrive $Y'c$ som $AYc$, så vi kan stryke og får $Yc' = \underline{f} \implies c(t) = \int_{0}^{t}Y^{-1}(s)\cdot \underline{f}(s)ds$

            \begin{Example}{}{}
                $\underline{y}' = \begin{bmatrix}
                    2 & 1 \\
                    1 & 2
                \end{bmatrix} \underline{y} + \begin{bmatrix}
                    1 \\
                    1
                \end{bmatrix}$ 

                Finner $Y(t) = \begin{bmatrix}
                    e^{3t} & e^{t} \\
                    e^{3t} & -e^{t}
                \end{bmatrix}$$

                $det(Y) = -2e^{4t}$

                $Y^{-1}(t) = -\frac{1}{2e^{4t}}\begin{bmatrix}
                    -e^t & -e^t \\
                    -e^{3t} & e^{3t}
                \end{bmatrix}$ 

                $Y^{-1}\underline{f}(t) = -\frac{1}{2e^{4t}}\begin{bmatrix}
                    -e^t & -e^t \\
                    -e^{3t} & e^{3t}
                \end{bmatrix}\begin{bmatrix}
                    1 \\
                    1
                \end{bmatrix} = \begin{bmatrix}
                e^{-3t} \\
                0
                \end{bmatrix}$

                $\int_{0}^{t}Y^{-1}\underline{f} = \begin{bmatrix}
                    -\frac{1}{3} e^{-3t} \\
                    0
                \end{bmatrix} = c(t)$

                $Yc = -\frac{1}{3}\begin{bmatrix}
                    1 \\
                    1
                \end{bmatrix}$ 
            \end{Example}

            \section{Repitisjon ig?}
            
            \subsection{Kont 2019}%
            \label{sub:kont_2019}
            
            \begin{Example}{Oppgave 1}{}
                a)


                Finn alle løsninger av \[ z^5 = i \] i $\mathbb{C}$.

                Setter opp likningen $z^5 = i = e^{i(\frac{\pi}{2} + 2n\pi)} \implies z = e^{\frac{i(\frac{\pi}{2} + 2n\pi)}{5}}$


                Deretter er det bare å sette inn verdier for $n$ hvor $0 \leq n \leq 4$


                b)


                La $z$ og $w$ være komplekse tall. Vis at \[ \bar{\left(\frac{z}{w}\right)} = \bar{z}/\bar{w} \]

                \begin{align*}
                    z &= a + bi \\
                    w &= c + di \\
                    \\
                    \\
                    \frac{z}{w} &= \frac{a + bi}{c + di}
                    \\
                                &= \frac{a+bi}{c + di} \cdot \frac{c-di}{c-di}
                                \\
                                &= \frac{ac + bd + i(bc - ad)}{c^2 + d^2}
                                \\
                                \\
                    \bar{\frac{z}{w}} &= \frac{ac + bd - i(bc - ad)}{c^2 + d^2}
                    \\
                    \\
                    \bar{z}/\bar{2} &= \frac{a-bi}{c - di} \cdot \frac{c+di}{c+di}
                    \\
                                    &= \frac{ac + bd -i(bc - ad)}{c^2 + d^2}
                \end{align*}
                Ser de er like :)
            \end{Example}

            \begin{Example}{Oppgave 2}{}
                a)

                
                La $A$ være en reell $m$x$n$-matrise. Skriv ned definisjonen på nullrommet til $A$. Vis at nullrommet er et underrom av $\mathbb{R}^{n}$.

                Nullrommet er alle $\underline{x}$ slik at $A\underline{x} = \underline{0}$. (Kjernen $T$ er det samme for lineærtransformasjoner). 

            Er det et vektorrom? Underrom? Et underrom er en delmengde som i seg selv er et vektorrom. 

            \begin{Theorem}{''Sjekketeoremet'' (7.9)}{}
                \begin{enumerate}
                    \item Er 0 med?
                    \item $\underline{x}$ og $\underline{y}$ er i delmengden, skal også $\underline{x} + \underline{y}$ være i delmengden
                    \item Dersom $\underline{x}$ er med, skal også $c\underline{x}$ være med.
                \end{enumerate}
            \end{Theorem}
                \begin{enumerate}
                    \item Sjekker om 0 er med. Ja, ganger vi $A$ med $0$ får vi $0$.
                    \item $A\underline{x} = 0$ og $A\underline{y} = 0 \implies A(\underline{x} + \underline{y}) = A\underline{x} + A\underline{y} = 0$. Med andre ord: $\underline{x}$ og $\underline{y}$ i nullrom impliserer at $\underline{x} + \underline{y}$ er i nullrommet.
                    \item $A\underline{x} = 0 \implies A(c\underline{x}) = 0 \implies c(A\underline{x}) = 0$
                \end{enumerate}

                 b)
                 
                Vi studerer matrisen \[ A = \begin{bmatrix}
                    2 & 4 & 0 \\
                    -5 & -4 & 6 \\
                    1 & -2 & -4
                \end{bmatrix} \]

                Ligger \[ \begin{bmatrix}
                    1 \\
                    2 \\
                    -1
                \end{bmatrix} \; \text{eller} \; \begin{bmatrix}
                    2 \\
                    -1 \\
                    1
                \end{bmatrix} \] i nullrommet til $A$?

                Finn en basis for $ColA$. Bestem dimensjonene på disse underrommene. 

                Ser med en gang at $\begin{bmatrix}
                    1 \\
                    2 \\
                    -1
                \end{bmatrix}$ ikke ligger i nullrommet. Ser at $A\cdot \begin{bmatrix}
                    2 \\
                    -1 \\
                    1
                \end{bmatrix} = 0$. Denne vektoren ligger i nullrommet. 


                Basiser:
                Radreduserer matrisen $A$:
                \begin{align*}
                    \begin{bmatrix}
                        2 & 4 & 0 \\
                        -5 & -4 & 6 \\
                        1 & -2 & -4
                    \end{bmatrix} &\sim \begin{bmatrix}
                        1 & 2 & 0 \\
                        0 & 6 & 6 \\
                        0 & -4 & -4
                    \end{bmatrix}
                    \\
                    &\sim \begin{bmatrix}
                        1 & 2 & 0 \\
                        0 & 1 & 1 \\
                        0 & 0 & 0
                    \end{bmatrix}
                    \\
                    \\
                    \implies ColA &= sp \left\{\begin{bmatrix}
                        2 \\
                        -5 \\
                        1
                    \end{bmatrix}, \begin{bmatrix}
                        4 \\
                        -4 \\
                        -2
                    \end{bmatrix}\right\}
                    \\
                    \\
                            \implies NullA &= sp \left\{\begin{bmatrix}
                                    2 \\
                                    -1 \\
                                    1
                            \end{bmatrix}\right\}
                \end{align*}
               \end{Example}
               \begin{Example}{Oppgave 2}{}
                
                c)
                Finn en ortogonal basis for $ColA$. Regn ut den ortogonale projeksjonen av $\begin{bmatrix}
                    3 \\
                    2 \\
                    9
                \end{bmatrix}$ ned på $ColA$.
                    
                Har basisen som ikke er ortogonal. Setter $\underline{u_1} = \begin{bmatrix}
                    2 \\
                    -5 \\
                    1
                \end{bmatrix}$ og bruker Grahmschmidt:
                \begin{align*}
                    \underline{u_2} &= \underline{v_2} - \frac{\underline{u_1} \cdot \underline{u_2}}{\underline{u_1} \cdot \underline{u_1}} \cdot \underline{u_1}
                    \\
                                    &= \begin{bmatrix}
                                        4 \\
                                        -4 \\
                                        -2
                                    \end{bmatrix} - \frac{2 \cdot 4 + (-5) \cdot (-4) + 1 \cdot (-2)}{2^2 + 5^2 + 1^2} \cdot \begin{bmatrix}
                                        2 \\
                                        -5 \\
                                        1
                                    \end{bmatrix}
                                    \\
                                    &= \begin{bmatrix}
                                        4 \\
                                        -4 \\
                                        -2
                                    \end{bmatrix} - \frac{26}{30} \cdot \begin{bmatrix}
                                        2 \\
                                        -5 \\
                                        1
                                    \end{bmatrix} = \frac{1}{30} \cdot \begin{bmatrix}
                                        68 \\
                                        10 \\
                                        -86
                                    \end{bmatrix}
                \end{align*}

                Projeksjon av $\begin{bmatrix}
                    3 \\
                    3 \\
                    9
                \end{bmatrix}$ ned på $ColA$:
                \begin{align*}
                    P\left(\begin{bmatrix}
                        3 \\
                        3 \\
                        9
                \end{bmatrix}\right) &= \frac{\begin{bmatrix}
                    3 \\
                    3 \\
                    9
                \end{bmatrix} \cdot \begin{bmatrix}
                    2 \\
                    -5 \\
                    1
                \end{bmatrix}}{\begin{bmatrix}
                    2 \\
                    -5 \\
                    1
                \end{bmatrix} \cdot \begin{bmatrix}
                    2 \\
                    -5 \\
                    1
                \end{bmatrix}} \cdot \begin{bmatrix}
                    2 \\
                    -5 \\
                    1
                \end{bmatrix} + \frac{\begin{bmatrix}
                    3 \\
                    3 \\
                    1
                \end{bmatrix} \cdot \begin{bmatrix}
                    68 \\
                    10 \\
                    -86
                \end{bmatrix}}{\begin{bmatrix}
                    68 \\
                    10 \\
                    -86
                \end{bmatrix} \cdot \begin{bmatrix}
                    68 \\
                    10 \\
                    -86
                \end{bmatrix}} \cdot \begin{bmatrix}
                    68 \\
                    10 \\
                    -86
                \end{bmatrix}
                \end{align*}
               \end{Example}
\end{document}
